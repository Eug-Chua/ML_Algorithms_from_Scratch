{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to evaluate a set of predictions on a classificaiton problem is by using accuracy.\n",
    "\n",
    "Classification accuracy tells us the number of correct predictions out of all the predictions that were made:\n",
    "\n",
    "Accuracy = $\\frac{\\text{correct predictions}}{\\text{total predictions}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy percentage between 2 lists\n",
    "def correct_counter(actual, predicted):\n",
    "\n",
    "    # set the container that stores correct matches to start with 0\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(len(actual)-1):\n",
    "        # correct container to add 1 if there is a match\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "\n",
    "    return round(correct /  len(actual) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.36\n"
     ]
    }
   ],
   "source": [
    "actual = [0,0,0,1,1,1,0,1,0,1,0]\n",
    "predicted = [1,0,1,0,0,1,0,0,1,1]\n",
    "\n",
    "accuracy = correct_counter(actual, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix produces a summary of all the predictions made compared to the actual values.\n",
    "\n",
    "The results are presented in a matrix with counts in each cell: the counts of predicted class values are summarized horizontally whereas the counts of actual values for each class values are presented vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(actual, predicted):\n",
    "    # get the unique classification labels\n",
    "    unique = set(actual)\n",
    "\n",
    "    # create an empty list that holds 1 list for each unique label\n",
    "    matrix = [list() for x in range(len(unique))]\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [0 for x in range(len(unique))]\n",
    "\n",
    "    # create empty dictionary\n",
    "    lookup = dict()\n",
    "\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        x = lookup[actual[i]]\n",
    "        y = lookup[predicted[i]]\n",
    "        matrix[x][y] += 1\n",
    "    return unique, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique labels: {0, 1, 2}\n",
      "matrix: [[1, 4, 0], [2, 1, 1], [2, 1, 3]]\n"
     ]
    }
   ],
   "source": [
    "actual = [0,2,0,2,1,1,0,2,0,1,2,0,1,2,2]\n",
    "predicted = [1,0,1,2,0,1,0,0,1,2,1,1,0,2,2]\n",
    "\n",
    "unique, matrix = confusion_matrix(actual, predicted)\n",
    "print(f'unique labels: {unique}')\n",
    "print(f'matrix: {matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error calculates the average of the absolute error values, where means are made absolute/positive so that they can be added together.\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{\\sum_{i=1}^{N} |\\text{predicted}_i - \\text{actual}_i|}{\\text{total predictions}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_metric(actual, predicted):\n",
    "    # initiate sum error as 0\n",
    "    sum_error = 0.0\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        # add to sum_error the absolute difference between predicted and actual values \n",
    "        sum_error += abs(predicted[i]- actual[i])\n",
    "    \n",
    "    # return the average value\n",
    "    return sum_error / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05000000000000001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = [1.55, 1.75, 1.3, 1.2, 1.85, 1.95]\n",
    "predicted = [1.6, 1.8, 1.25, 1.15, 1.9, 1.9]\n",
    "\n",
    "mae = mae_metric(actual, predicted)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful metric to calculate the error in a set of regression predictions is to use the Root Mean Squared Error.\n",
    "\n",
    "It is calculated as the square root of the mean of the squared differences between the actual and predicted outcomes.\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{\\sum_{i=1}^{N} (\\text{predicted}_i - \\text{actual}_i)^2}{\\text{total predictions}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_metric(actual, predicted):\n",
    "    # initiate sum error as 0\n",
    "    sum_error = 0.0\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        # get the difference between the predicted and actual values\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "\n",
    "        # cumulatively add the difference to sum_error\n",
    "        sum_error += prediction_error ** 2\n",
    "        \n",
    "    mean_error = sum_error / len(actual)\n",
    "    return mean_error ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05000000000000001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = [1.55, 1.75, 1.3, 1.2, 1.85, 1.95]\n",
    "predicted = [1.6, 1.8, 1.25, 1.15, 1.9, 1.9]\n",
    "\n",
    "rmse = rmse_metric(actual, predicted)\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
